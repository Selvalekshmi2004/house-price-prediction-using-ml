import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
import matplotlib.pyplot as plt
import ssl
import urllib.request
ssl._create_default_https_context = ssl._create_unverified_context


# Load dataset

from sklearn.datasets import fetch_california_housing
california = fetch_california_housing()
data = pd.DataFrame(california.data, columns=california.feature_names)
data['PRICE'] = california.target


# Data Exploration
print(data.head())
print(data.info())
print(data.describe())

# Check for missing values
print("Missing values:")
print(data.isnull().sum())

# Data Preprocessing
X = data.drop(columns=['PRICE'])
y = data['PRICE']

# Normalize features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Model Building
# Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Ridge Regression with Hyperparameter Tuning
ridge_model = Ridge()
param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}
ridge_cv = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')
ridge_cv.fit(X_train, y_train)

# Best Ridge Regression model
best_ridge_model = ridge_cv.best_estimator_
print(f"Best alpha for Ridge Regression: {ridge_cv.best_params_['alpha']}")

# Predictions
linear_preds = linear_model.predict(X_test)
ridge_preds = best_ridge_model.predict(X_test)

# Evaluation
linear_mse = mean_squared_error(y_test, linear_preds)
ridge_mse = mean_squared_error(y_test, ridge_preds)
linear_r2 = r2_score(y_test, linear_preds)
ridge_r2 = r2_score(y_test, ridge_preds)

print("Linear Regression Results:")
print(f"MSE: {linear_mse:.2f}, R2: {linear_r2:.2f}")

print("Ridge Regression Results:")
print(f"MSE: {ridge_mse:.2f}, R2: {ridge_r2:.2f}")

# Feature Importance for Linear Regression
coefficients = pd.DataFrame({'Feature': california.feature_names, 'Coefficient': linear_model.coef_})
coefficients = coefficients.sort_values(by='Coefficient', ascending=False)

# Visualization
plt.figure(figsize=(10, 6))
sns.barplot(x='Coefficient', y='Feature', data=coefficients)
plt.title("Feature Importance in Linear Regression")
plt.show()


# Conclusion
if ridge_r2 > linear_r2:
    print("Ridge Regression performs better.")
else:
    print("Linear Regression performs better.")
